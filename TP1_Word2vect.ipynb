{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H2qne3bHeVR-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"
      ],
      "metadata": {
        "id": "Yusii1tDeZV2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias', 'qué dia es mañana', 'que día es'])"
      ],
      "metadata": {
        "id": "L90zYruAebnT"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n",
        "- Cada documento transformarlo en una lista de términos\n",
        "- Armar un vector de términos no repetidos de todos los documentos"
      ],
      "metadata": {
        "id": "FnFI-qOVCjIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cada documento transformarlo en una lista de términos\n",
        "docs = []\n",
        "for document in corpus:\n",
        "  document = document.split(\" \")\n",
        "  docs.append(document)"
      ],
      "metadata": {
        "id": "08nIql8DeeWW"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Armar un vector de términos no repetidos de todos los documentos\n",
        "split_corpus = np.char.split(corpus, sep =' ') \n",
        "all_tokens = np.sum(split_corpus)\n",
        "unique_tokens = np.unique(all_tokens)\n",
        "#unique_tokens"
      ],
      "metadata": {
        "id": "KZRuvmWSegjx"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generar diccionario de palabras para usar más adelante\n",
        "vocab = {}\n",
        "index = 0\n",
        "for document in docs:\n",
        "  for word in document:\n",
        "    if not word in vocab:\n",
        "      vocab[word] = index\n",
        "      index +=1"
      ],
      "metadata": {
        "id": "NnXYyz16Cyqc"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh9LELijDgj1",
        "outputId": "a16d3876-8836-4606-f73c-65fe2759cc94"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'que': 0,\n",
              " 'dia': 1,\n",
              " 'es': 2,\n",
              " 'hoy': 3,\n",
              " 'martes': 4,\n",
              " 'el': 5,\n",
              " 'de': 6,\n",
              " 'muchas': 7,\n",
              " 'gracias': 8,\n",
              " 'qué': 9,\n",
              " 'mañana': 10,\n",
              " 'día': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2- OneHot encoding\n",
        "Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"
      ],
      "metadata": {
        "id": "IRNBKdeNemDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creo matrix con 1s en la diagonal y 0 en el resto\n",
        "onehot_token = np.eye(len(vocab))\n",
        "\n",
        "onehot_matrix = []\n",
        "for document in docs:\n",
        "  onehot_doc = np.zeros((len(document), len(vocab)))\n",
        "  for word in range(len(document)):\n",
        "    onehot_doc[word, :] = onehot_token[vocab[document[word]]]\n",
        "  onehot_matrix.append(np.array(onehot_doc))"
      ],
      "metadata": {
        "id": "PMWcuugsE-Wr"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utaxQ4IUFCAi",
        "outputId": "f6c3a244-952e-4bfb-cf09-654c6f89970a"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
              " array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
              " array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\n",
              " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]),\n",
              " array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3- Vectores de frecuencia\n",
        "Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"
      ],
      "metadata": {
        "id": "pYRk8g0YhNNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_freq = []\n",
        "for document in onehot_matrix:\n",
        "  count_freq.append(document.sum(axis=0))\n",
        "\n",
        "freq_dist = np.array(count_freq)"
      ],
      "metadata": {
        "id": "A3u9b1fu9hEc"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMXLZLzkG2MO",
        "outputId": "acf2b542-f051-4550-d22a-4da3d00c1dc1"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 2., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "       [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4- TF-IDF\n",
        "Data una lista de textos, devolver una matriz con la representacion TFIDF"
      ],
      "metadata": {
        "id": "DoiIn3ub8G__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vocab = np.zeros(len(vocab))\n",
        "for token in vocab:\n",
        "  for document in docs:\n",
        "    if token in document:\n",
        "      df_vocab[vocab[token]] +=1                   "
      ],
      "metadata": {
        "id": "Vg0z2N-sBvuh"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4g93V-kDzz0",
        "outputId": "fce400de-f7bf-46d7-a24f-19d1b8cb5be3"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 3., 4., 2., 2., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_corpus = len(corpus)"
      ],
      "metadata": {
        "id": "awijfXXDDz4p"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf = np.log10(len_corpus/df_vocab)"
      ],
      "metadata": {
        "id": "0NGuBvSrD5ms"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf = np.zeros((len_corpus, len(vocab)))\n",
        "for i in range(len_corpus):\n",
        "  tf_idf[i,:] = np.multiply(idf, freq_dist[i])\n",
        "\n",
        "tf_idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6yadfzbEAOe",
        "outputId": "f26bf349-04ed-43fb-aad1-627f0a13f357"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39794001, 0.22184875, 0.09691001, 0.39794001, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.22184875, 0.09691001, 0.39794001, 0.79588002,\n",
              "        0.69897   , 0.69897   , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.39794001,\n",
              "        0.        , 0.        , 0.69897   , 0.69897   , 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.22184875, 0.09691001, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.69897   ,\n",
              "        0.69897   , 0.        ],\n",
              "       [0.39794001, 0.        , 0.09691001, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.69897   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparación de documentos\n",
        "Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"
      ],
      "metadata": {
        "id": "ZGxl3aXIALyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_comparison(corpus, idx):\n",
        "\n",
        "  #SEPARACIÓN DE DOCUMENTOS EN LISTAS DE TÉRMINOS\n",
        "  docs = []\n",
        "  for document in corpus:\n",
        "    document = document.split(\" \")\n",
        "    docs.append(document)\n",
        "\n",
        "  #DICCIONARIO DE PALABRAS\n",
        "  vocab = {}\n",
        "  index = 0\n",
        "  for document in docs:\n",
        "    for word in document:\n",
        "      if not word in vocab:\n",
        "        vocab[word] = index\n",
        "        index +=1\n",
        "\n",
        "  #ONE HOT ENCODING\n",
        "  onehot_token = np.eye(len(vocab))\n",
        "\n",
        "  onehot_matrix = []\n",
        "  for document in docs:\n",
        "    onehot_doc = np.zeros((len(document), len(vocab)))\n",
        "    for word in range(len(document)):\n",
        "      onehot_doc[word, :] = onehot_token[vocab[document[word]]]\n",
        "    onehot_matrix.append(np.array(onehot_doc)) \n",
        "\n",
        "  #COUNT FREQUENCY\n",
        "  count_freq = []\n",
        "  for document in onehot_matrix:\n",
        "    count_freq.append(document.sum(axis=0))\n",
        "  freq_dist = np.array(count_freq)\n",
        "\n",
        "  #TF IDF\n",
        "  #1 - frecuencia de las palabras en el corpus\n",
        "  df_vocab = np.zeros(len(vocab))\n",
        "  for token in vocab:\n",
        "    for document in docs:\n",
        "      if token in document:\n",
        "        df_vocab[vocab[token]] +=1                   \n",
        "  \n",
        "  #2. frecuencia inversa de las palabras\n",
        "  len_corpus = len(corpus)\n",
        "  idf = np.log10(len_corpus/df_vocab) \n",
        "\n",
        "  #3. indice tf.idf = element wise multiplication\n",
        "  tf_idf = np.zeros((len_corpus, len(vocab)))\n",
        "  for i in range(len_corpus):\n",
        "    tf_idf[i,:] = np.multiply(idf, freq_dist[i])\n",
        "\n",
        "  similarity = np.zeros(len_corpus)\n",
        "  for i in range(len_corpus):\n",
        "    similarity[i] = cosine_similarity(tf_idf[i,:], tf_idf[idx, :])\n",
        "\n",
        "  return corpus[np.argsort(-similarity)]"
      ],
      "metadata": {
        "id": "8cYjOYkQAYvo"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_comparison(corpus, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg656l4CHeab",
        "outputId": "e4334814-cd17-4213-961b-c7f2b2b2c3a8"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['martes el dia de hoy es martes', 'que dia es hoy',\n",
              "       'martes muchas gracias', 'qué dia es mañana', 'que día es'],\n",
              "      dtype='<U30')"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eT_Lu8TbHfy5"
      },
      "execution_count": 129,
      "outputs": []
    }
  ]
}